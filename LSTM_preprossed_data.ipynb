{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['OSD.flyTime [s]', 'HOME.distance [ft]', 'OSD.height [ft]',\n",
      "       'OSD.longitude', 'OSD.latitude', 'OSD.altitude [ft]'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7374 entries, 0 to 7373\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   OSD.flyTime [s]     7374 non-null   float64\n",
      " 1   HOME.distance [ft]  7374 non-null   float64\n",
      " 2   OSD.height [ft]     7374 non-null   float64\n",
      " 3   OSD.longitude       7374 non-null   float64\n",
      " 4   OSD.latitude        7374 non-null   float64\n",
      " 5   OSD.altitude [ft]   7374 non-null   int64  \n",
      "dtypes: float64(5), int64(1)\n",
      "memory usage: 345.8 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the CSV file and filter numeric columns\n",
    "df= pd.read_csv(r'preprocced_dataset.csv', low_memory=False)\n",
    "\n",
    "print(df.columns)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = df[[ 'OSD.flyTime [s]', 'HOME.distance [ft]', 'OSD.height [ft]']]\n",
    "\n",
    "target_columns = df[['OSD.longitude', 'OSD.latitude','OSD.altitude [ft]']]\n",
    "\n",
    "# Initialize the scaler and normalize the features and target\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(feature_columns)\n",
    "y_scaled = scaler_y.fit_transform(target_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old sequencing technique (Used in each previous code)\n",
    "\n",
    "def create_sequences(X, y, sequence_length):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - sequence_length):\n",
    "        Xs.append(X[i:(i + sequence_length)])\n",
    "        ys.append(y[i + sequence_length])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "#PARAS\n",
    "sequence_length = 50  #LOOKBACKS\n",
    "batch_size = 32\n",
    "\n",
    "#SEQ\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7324, 50, 3)\n",
      "(7324, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_seq.shape)\n",
    "print(y_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Determine the split indices based on percentage\\ntrain_size = int(0.7 * len(X_seq))  # 70% for training\\nval_size = int(0.15 * len(X_seq))    # 15% for validation\\n\\n# Train: First 70%\\nX_train, y_train = X_seq[:train_size], y_seq[:train_size]\\n\\n# Validation: Next 15% (from train_size to train_size + val_size)\\nX_val, y_val = X_seq[train_size:train_size + val_size], y_seq[train_size:train_size + val_size]\\n\\n# Test: Final 15% (from train_size + val_size to the end)\\nX_test, y_test = X_seq[train_size: ], y_seq[train_size:]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Determine the split indices based on percentage\n",
    "train_size = int(0.7 * len(X_seq))  # 70% for training\n",
    "val_size = int(0.15 * len(X_seq))    # 15% for validation\n",
    "\n",
    "# Train: First 70%\n",
    "X_train, y_train = X_seq[:train_size], y_seq[:train_size]\n",
    "\n",
    "# Validation: Next 15% (from train_size to train_size + val_size)\n",
    "X_val, y_val = X_seq[train_size:train_size + val_size], y_seq[train_size:train_size + val_size]\n",
    "\n",
    "# Test: Final 15% (from train_size + val_size to the end)\n",
    "X_test, y_test = X_seq[train_size: ], y_seq[train_size:]'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN , VALIDATION AND TESTING AREA\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_seq, y_seq, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1099, 50, 3)\n",
      "(1099, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5126, 50, 3)\n",
      "y_train shape: (5126, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)  # Should be (num_samples, sequence_length, n_features)\n",
    "print(\"y_train shape:\", y_train.shape)  # Should be (num_samples, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSTM model\n",
    "def build_model(sequence_length, n_features):\n",
    "    model = Sequential([\n",
    "        # First LSTM layer\n",
    "        LSTM(128, return_sequences=True, input_shape=(sequence_length, n_features)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Second LSTM layer\n",
    "        LSTM(64, return_sequences=True),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Third LSTM layer\n",
    "        LSTM(32),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Dense layers for prediction\n",
    "        Dense(16, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(3)  # 2 outputs: latitude and longitude and altitute\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALLBACKS (will stop overfitting when patience level reached)\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_lstm_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=0.00001\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5126, 50, 3)\n",
      "y_train shape: (5126, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vines\\kanikamtech\\Anuj_test\\College_Drone_November\\7th_Nov\\.venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Verify shapes\n",
    "print(\"X_train shape:\", X_train.shape)  # Expected: (num_samples, sequence_length, n_features)\n",
    "print(\"y_train shape:\", y_train.shape)  # Expected: (num_samples, 2) if predicting latitude and longitude\n",
    "\n",
    "# Define sequence length and number of features\n",
    "sequence_length = X_train.shape[1] # Should be set correctly in the dataset\n",
    "n_features = X_train.shape[2]\n",
    "\n",
    "# Initialize model with correct dimensions\n",
    "model = build_model(sequence_length, n_features)\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.6595 - mae: 0.6402 - val_loss: 0.1639 - val_mae: 0.3375 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - loss: 0.0890 - mae: 0.2367 - val_loss: 0.0234 - val_mae: 0.1152 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step - loss: 0.0329 - mae: 0.1405 - val_loss: 0.0091 - val_mae: 0.0669 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step - loss: 0.0249 - mae: 0.1205 - val_loss: 0.0055 - val_mae: 0.0607 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - loss: 0.0241 - mae: 0.1183 - val_loss: 0.0098 - val_mae: 0.0722 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - loss: 0.0243 - mae: 0.1190 - val_loss: 0.0151 - val_mae: 0.0903 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - loss: 0.0136 - mae: 0.0894 - val_loss: 0.0048 - val_mae: 0.0518 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 30ms/step - loss: 0.0131 - mae: 0.0868 - val_loss: 0.0079 - val_mae: 0.0645 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - loss: 0.0119 - mae: 0.0837 - val_loss: 0.0028 - val_mae: 0.0428 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step - loss: 0.0119 - mae: 0.0829 - val_loss: 0.0033 - val_mae: 0.0422 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - loss: 0.0091 - mae: 0.0725 - val_loss: 0.0011 - val_mae: 0.0244 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step - loss: 0.0086 - mae: 0.0695 - val_loss: 0.0014 - val_mae: 0.0303 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - loss: 0.0078 - mae: 0.0666 - val_loss: 0.0063 - val_mae: 0.0550 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - loss: 0.0087 - mae: 0.0692 - val_loss: 0.0020 - val_mae: 0.0342 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step - loss: 0.0082 - mae: 0.0682 - val_loss: 0.0023 - val_mae: 0.0330 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step - loss: 0.0070 - mae: 0.0629 - val_loss: 0.0063 - val_mae: 0.0501 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step - loss: 0.0076 - mae: 0.0652 - val_loss: 4.7516e-04 - val_mae: 0.0171 - learning_rate: 5.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - loss: 0.0064 - mae: 0.0610 - val_loss: 0.0012 - val_mae: 0.0244 - learning_rate: 5.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - loss: 0.0067 - mae: 0.0607 - val_loss: 6.5609e-04 - val_mae: 0.0189 - learning_rate: 5.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - loss: 0.0071 - mae: 0.0622 - val_loss: 0.0060 - val_mae: 0.0514 - learning_rate: 5.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - loss: 0.0067 - mae: 0.0621 - val_loss: 8.4265e-04 - val_mae: 0.0193 - learning_rate: 5.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - loss: 0.0064 - mae: 0.0604 - val_loss: 6.6625e-04 - val_mae: 0.0191 - learning_rate: 5.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - loss: 0.0061 - mae: 0.0580 - val_loss: 0.0013 - val_mae: 0.0252 - learning_rate: 2.5000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step - loss: 0.0062 - mae: 0.0594 - val_loss: 0.0016 - val_mae: 0.0288 - learning_rate: 2.5000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - loss: 0.0062 - mae: 0.0594 - val_loss: 6.1900e-04 - val_mae: 0.0180 - learning_rate: 2.5000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - loss: 0.0067 - mae: 0.0598 - val_loss: 0.0021 - val_mae: 0.0312 - learning_rate: 2.5000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step - loss: 0.0067 - mae: 0.0612 - val_loss: 0.0015 - val_mae: 0.0270 - learning_rate: 2.5000e-04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1099, 50, 3)\n",
      "(1099, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 0.000473\n",
      "Test MAE: 0.017121\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nTest Loss: {test_loss:.6f}')\n",
    "print(f'Test MAE: {test_mae:.6f}')\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_test_orig = scaler_y.inverse_transform(y_test)\n",
    "y_pred_orig = scaler_y.inverse_transform(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Latitude MAE: 0.001173 degrees\n",
      "Longitude MAE: 0.000500 degrees\n",
      "Altitute MAE: 2980.844357 ft\n",
      "\n",
      "Prediction Results Summary:\n",
      "Average Latitude Error: 0.001173 degrees\n",
      "Average Longitude Error: 0.000500 degrees\n",
      "Max Latitude Error: 0.003453 degrees\n",
      "Max Longitude Error: 0.002362 degrees\n"
     ]
    }
   ],
   "source": [
    "# ERROR PART\n",
    "mae_lat = np.mean(np.abs(y_test_orig[:, 0] - y_pred_orig[:, 0]))\n",
    "mae_lon = np.mean(np.abs(y_test_orig[:, 1] - y_pred_orig[:, 1]))\n",
    "mae_alt = np.mean(np.abs(y_test_orig[:, 2] - y_pred_orig[:, 1]))\n",
    "print(f'\\nLatitude MAE: {mae_lat:.6f} degrees')\n",
    "print(f'Longitude MAE: {mae_lon:.6f} degrees')\n",
    "print(f'Altitute MAE: {mae_alt:.6f} ft')\n",
    "\n",
    "# HISTORY OF EVERY TRAINING PART (PLEASE CHECK THE IMAGES CREATED)\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Model MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png')\n",
    "plt.close()\n",
    "\n",
    "# LATITUDE AND LONGITUDE ( Actul vs Predicted )\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test_orig[:, 0], y_pred_orig[:, 0], marker='o')\n",
    "plt.plot([y_test_orig[:, 0].min(), y_test_orig[:, 0].max()], \n",
    "         [y_test_orig[:, 0].min(), y_test_orig[:, 0].max()], \n",
    "         'r--', lw=2)\n",
    "plt.title('Predicted vs Actual Latitude')\n",
    "plt.xlabel('Actual Latitude')\n",
    "plt.ylabel('Predicted Latitude')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_test_orig[:, 1], y_pred_orig[:, 1],marker='o')\n",
    "plt.plot([y_test_orig[:, 1].min(), y_test_orig[:, 1].max()], \n",
    "         [y_test_orig[:, 1].min(), y_test_orig[:, 1].max()], \n",
    "         'r--', lw=2)\n",
    "\n",
    "plt.title('Predicted vs Actual Longitude')\n",
    "plt.xlabel('Actual Longitude')\n",
    "plt.ylabel('Predicted Longitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('prediction_results.png')\n",
    "plt.close()\n",
    "\n",
    "# ACTUAL VS PREDICTED PATH\n",
    "plt.figure(figsize=(12, 8))\n",
    "#plt.plot(y_test_orig[:, 1], y_test_orig[:, 0], 'b-', label='Actual Path', alpha=0.7,)\n",
    "#plt.plot(y_pred_orig[:, 1], y_pred_orig[:, 0], 'r--', label='Predicted Path', alpha=0.7)\n",
    "plt.scatter(y_test_orig[:, 1], y_test_orig[:, 0], color='b', label='Actual Path',marker='o')\n",
    "plt.scatter(y_pred_orig[:, 1], y_pred_orig[:, 0], color='r', label='Predicted Path',marker='o')\n",
    "plt.title('Actual vs Predicted Path')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('path_comparison.png')\n",
    "plt.close()\n",
    "\n",
    "# RUN THIS PART IN CASE YOU WANT TO SAVE THE PREDICTION IN A NEW CSV\n",
    "results_df = pd.DataFrame({\n",
    "     'Actual_Latitude': y_test_orig[:, 0],\n",
    "     'Actual_Longitude': y_test_orig[:, 1],\n",
    "     'Predicted_Latitude': y_pred_orig[:, 0],\n",
    "     'Predicted_Longitude': y_pred_orig[:, 1],\n",
    "     'Latitude_Error': np.abs(y_test_orig[:, 0] - y_pred_orig[:, 0]),\n",
    "     'Longitude_Error': np.abs(y_test_orig[:, 1] - y_pred_orig[:, 1])\n",
    " })\n",
    "results_df.to_csv('prediction_results.csv', index=False)\n",
    "\n",
    "print(\"\\nPrediction Results Summary:\")\n",
    "print(f\"Average Latitude Error: {results_df['Latitude_Error'].mean():.6f} degrees\")\n",
    "print(f\"Average Longitude Error: {results_df['Longitude_Error'].mean():.6f} degrees\")\n",
    "print(f\"Max Latitude Error: {results_df['Latitude_Error'].max():.6f} degrees\")\n",
    "print(f\"Max Longitude Error: {results_df['Longitude_Error'].max():.6f} degrees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTUAL VS PREDICTED PATH\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(y_test_orig[:, 1], y_test_orig[:, 0], 'b-', label='Actual Path', alpha=0.7)\n",
    "#plt.plot(y_pred_orig[:, 1], y_pred_orig[:, 0], 'r--', label='Predicted Path', alpha=0.7)\n",
    "plt.title('Actual  Path')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('actual.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTUAL VS PREDICTED PATH\n",
    "plt.figure(figsize=(12, 8))\n",
    "#plt.plot(y_test_orig[:, 1], y_test_orig[:, 0], 'b-', label='Actual Path', alpha=0.7)\n",
    "plt.plot(y_pred_orig[:, 1], y_pred_orig[:, 0], 'r--', label='Predicted Path', alpha=0.7)\n",
    "plt.title(' Predicted Path')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('predicted.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
